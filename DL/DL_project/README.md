# A foolproof way to shrink deep learning models
## Researchers unveil a pruning algorithm to make artificial intelligence applications run faster.
### Kim Martineau ( MIT Quest for Intelligence)
### April 30, 2020    


<img src="/home/amy/DL/DL_project/img1.png"></img>    

As more artificial intelligence applications move to smartphones, deep learning models are getting smaller to allow apps to run faster and save battery power. Now, MIT researchers have a new and better way to compress models.    
    
Itâ€™s so simple that they unveiled it in a tweet last month: Train the model, prune its weakest connections, retrain the model at its fast, early training rate, and repeat, until the model is as tiny as you want.    
    




 
